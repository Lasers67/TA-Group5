{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import json\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "language = 'en'\n",
    "start = 0 if language!='fr' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_model(language):\n",
    "    if language == 'EN':\n",
    "        return spacy.load(\"en_core_web_md\")\n",
    "    elif language == 'FR':\n",
    "        return spacy.load(\"fr_core_news_sm\")\n",
    "    elif language == 'ES':\n",
    "        return spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {language}\")\n",
    "        \n",
    "def extract_text_from_pdf(pdf_path, start = 0):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages[start:]:\n",
    "        text += page.extract_text() + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    return pos_counts\n",
    "def get_author_name(text):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=\"timpal0l/mdeberta-v3-base-squad2\")\n",
    "    question = \"Qui est l'auteur de l'article?\"\n",
    "    result = qa_pipeline(question=question, context=text[:500])\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_preprocessed_text_from_pdf(pdf_path, language='EN', start=0):\n",
    "    lines = extract_text_from_pdf(pdf_path, start).split('\\n')\n",
    "    pre_processed_text = []\n",
    "    within_abstract = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if language != 'EN':\n",
    "            pre_processed_text.append(line)\n",
    "        else:\n",
    "            if line.lower() == 'abstract':\n",
    "                within_abstract = True\n",
    "                continue\n",
    "            elif line.lower() == 'references':\n",
    "                within_abstract = False\n",
    "\n",
    "            if within_abstract:\n",
    "                pre_processed_text.append(line)\n",
    "    return ' '.join(pre_processed_text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, language=\"EN\"):\n",
    "    nltk_lang_map = {\n",
    "        \"EN\": \"english\",\n",
    "        \"FR\": \"french\",\n",
    "        \"ES\": \"spanish\"\n",
    "    }\n",
    "    lang_code = nltk_lang_map.get(language.upper(), \"english\")\n",
    "    return word_tokenize(text, language=lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, language=\"EN\"):\n",
    "    nltk_lang_map = {\n",
    "        \"EN\": \"english\",\n",
    "        \"FR\": \"french\",\n",
    "        \"ES\": \"spanish\"\n",
    "    }\n",
    "    lang_code = nltk_lang_map.get(language.upper(), \"english\")\n",
    "    return word_tokenize(text, language=lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens) if tokens else 0\n",
    "\n",
    "def calculate_hapax_legomena(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    word_counts = Counter(tokens)\n",
    "    hapax_legomena = sum(1 for word in word_counts if word_counts[word] == 1)\n",
    "    return hapax_legomena / len(tokens) if tokens else 0\n",
    "\n",
    "def calculate_msl(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    total_words = len([token.text for token in doc if not token.is_punct])\n",
    "    return total_words / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#involvement metrics\n",
    "def calculate_involvement_rate(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    total_words = len([token for token in doc if not token.is_punct])\n",
    "    personal_pronouns = sum(1 for token in doc if token.pos_ == \"PRON\" and token.tag_ in {\"PRP\", \"PRP$\"})\n",
    "    questions = sum(1 for token in doc if token.text == \"?\")\n",
    "    coordination = sum(1 for token in doc if token.dep_ == \"cc\")\n",
    "    return (personal_pronouns + questions + coordination) / total_words if total_words else 0\n",
    "\n",
    "def calculate_informational_rate(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    total_words = len([token for token in doc if not token.is_punct])\n",
    "    noun_phrases = len(list(doc.noun_chunks))\n",
    "    technical_terms = sum(1 for token in doc if token.pos_ == \"NOUN\" and len(token.text) > 6)\n",
    "    references = sum(1 for token in doc if token.like_num or token.text.startswith(\"http\") or token.text.startswith(\"@\"))\n",
    "    return (noun_phrases + technical_terms + references) / total_words if total_words else 0\n",
    "\n",
    "def calculate_iir(text, nlp):\n",
    "    involvement = calculate_involvement_rate(text, nlp)\n",
    "    informational = calculate_informational_rate(text, nlp)\n",
    "    return involvement / informational if informational else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Metadata-GENDER.csv\")\n",
    "df['FileName'] = df['FileName'].astyp(str)\n",
    "df['Paper_Language'] = df['Paper_Language'].astyp(str)\n",
    "df['Paper_Subject'] = df['Paper_Subject'].astyp(str)\n",
    "df['corpus'] = \"\"\n",
    "df['ttr'] = np.nan\n",
    "df['hapax'] = np.nan\n",
    "df['msl'] = np.nan\n",
    "df['involvement_rate'] = np.nan\n",
    "df['informational_rate'] = np.nan\n",
    "df['iir'] = np.nan\n",
    "\n",
    "def process_pdfs_in_folder(folder_path, lang, sub):\n",
    "    i = 1\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            match = re.search(r'(\\d+)', filename)\n",
    "            if match:\n",
    "                file_number = str(match.group(1))\n",
    "                print(f\"Extracted file number: {i}\")\n",
    "\n",
    "                row = df[(df['FileName'].str.lower() == file_number.lower()) &\n",
    "                         (df['Paper_Language'].str.lower() == lang.lower()) &\n",
    "                         (df['Paper_Subject'].str.lower() == sub.lower())]\n",
    "                nlp = load_spacy_model(lang)\n",
    "                if not row.empty:\n",
    "                    pdf_path = os.path.join(folder_path, filename)\n",
    "                    pre_processed_text = return_preprocessed_text_from_pdf(pdf_path, lang)\n",
    "                    df.loc[row.index, 'corpus'] = pre_processed_text\n",
    "                    df['ttr'] = calculate_ttr(text)\n",
    "                    df['hapax'] = calculate_hapax_legomena(text)\n",
    "                    df['msl'] = calculate_msl(text, nlp)\n",
    "                    df.loc[row.index, 'involvement_rate'] = calculate_involvement_rate(pre_processed_text, nlp)\n",
    "                    df.loc[row.index, 'informational_rate'] = calculate_informational_rate(pre_processed_text, nlp)\n",
    "                    df.loc[row.index, 'iir'] = calculate_iir(pre_processed_text, nlp)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/EN-His\"\n",
    "print(\"EN-HIS\")\n",
    "process_pdfs_in_folder(folder_path, \"EN\", \"HIS\")\n",
    "\n",
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/EN-Psy\"\n",
    "print(\"EN-PSY\")\n",
    "process_pdfs_in_folder(folder_path, \"EN\", \"PSY\")\n",
    "\n",
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/FR-His\"\n",
    "print(\"FR-HIS\")\n",
    "process_pdfs_in_folder(folder_path, \"FR\", \"HIS\")\n",
    "\n",
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/FR-Psy\"\n",
    "print(\"FR-PSY\")\n",
    "process_pdfs_in_folder(folder_path, \"FR\", \"PSY\")\n",
    "\n",
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/ES-His\"\n",
    "print(\"ES-HIS\")\n",
    "process_pdfs_in_folder(folder_path, \"ES\", \"HIS\")\n",
    "\n",
    "folder_path = \"D:/Text Analytics/Term Paper/dataset/ES-Psy\"\n",
    "print(\"ES-PSY\")\n",
    "process_pdfs_in_folder(folder_path, \"ES\", \"PSY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"D:\\Text Analytics\\Term Paper\\metrics_output.csv\"\n",
    "df_without_corpus = df.drop(columns=['corpus'])\n",
    "df_without_corpus.to_csv(output_file_path, index=False)\n",
    "print(f\"DataFrame saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphs for involvement metrics\n",
    "df_without_corpus = pd.read_csv(\"metrics_output.csv\")\n",
    "df_cleaned = df_without_corpus.replace([np.inf, -np.inf], np.nan)\n",
    "df_cleaned = df_cleaned.dropna(subset=['ttr', 'hapax', 'msl', 'involvement_rate', 'informational_rate', 'iir'])\n",
    "df_cleaned = df_cleaned[(df_cleaned[['ttr', 'hapax', 'msl', 'involvement_rate', 'informational_rate', 'iir']] != 0).all(axis=1)]\n",
    "sns.set(style=\"whitegrid\")\n",
    "gender_palette = {'F': '#FF69B4', 'M': '#1E90FF'}\n",
    "\n",
    "def create_violin_subplots(df_cleaned, category, typ, choice, metrics):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        sns.violinplot(x=category, y=metric, hue=\"GENDER\", data=df_cleaned, split=True, palette=gender_palette, ax=axes[i])\n",
    "        axes[i].set_title(f\"{metric.upper()} by {category}\")\n",
    "        axes[i].set_ylabel(metric.replace('_', ' ').title())\n",
    "        axes[i].set_xlabel(category)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"violin-{typ}-{choice}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def create_bar_subplots(df_cleaned, category, typ, choice, metrics):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        df_grouped = df_cleaned.groupby([category, 'GENDER'])[metric].mean().reset_index()\n",
    "        sns.barplot(x=category, y=metric, hue='GENDER', data=df_grouped, ax=axes[i], palette=gender_palette        )\n",
    "        axes[i].set_title(f\"{metric.upper()} by {category}\")\n",
    "        axes[i].set_ylabel(metric.replace('_', ' ').title())\n",
    "        axes[i].set_xlabel(category)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"bar-{typ}-{choice}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def create_single_plot(df_cleaned, category, typ, choice, metric, plot_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if plot_type == 'violin':\n",
    "        sns.violinplot(x=category, y=metric, hue=\"GENDER\", data=df_cleaned, split=True, palette=gender_palette)\n",
    "    else:\n",
    "        df_grouped = df_cleaned.groupby([category, 'GENDER'])[metric].mean().reset_index()\n",
    "        sns.barplot(x=category, y=metric, hue='GENDER', data=df_grouped, palette=gender_palette)\n",
    "    plt.title(f\"{metric.upper()} by {category}\")\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.xlabel(category)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_type}-{typ}-{choice}-{metric}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def call_with_metrics(choice):\n",
    "    if choice == 'all':\n",
    "        metrics = ['involvement_rate', 'informational_rate', 'iir']\n",
    "        for category, typ in [('Paper_Language', 'l'), ('Paper_Subject', 's')]:\n",
    "            create_violin_subplots(df_cleaned, category, typ, choice, metrics)\n",
    "            create_bar_subplots(df_cleaned, category, typ, choice, metrics)\n",
    "    else:\n",
    "        metric = 'iir'\n",
    "        for category, typ in [('Paper_Language', 'l'), ('Paper_Subject', 's')]:\n",
    "            create_single_plot(df_cleaned, category, typ, choice, metric, plot_type='violin')\n",
    "            create_single_plot(df_cleaned, category, typ, choice, metric, plot_type='bar')\n",
    "\n",
    "call_with_metrics('all')\n",
    "call_with_metrics('one')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
