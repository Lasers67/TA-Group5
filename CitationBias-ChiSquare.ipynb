{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import json\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "language = 'en'\n",
    "start = 0 if language!='fr' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, start = 0):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages[start:]:\n",
    "        text += page.extract_text() + \" \"\n",
    "    return text\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    return pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_preprocessed_text_from_pdf(pdf_path):\n",
    "    t = extract_text_from_pdf(pdf_path, start).split('\\n')\n",
    "    pre_processed_text = []\n",
    "    index = -1\n",
    "    for i in t:\n",
    "        if language!= 'en':\n",
    "            pre_processed_text.append(i)\n",
    "        else:\n",
    "            if index ==0:\n",
    "                pre_processed_text.append(i)\n",
    "            if i=='Abstract':\n",
    "                index = 0\n",
    "            if i=='References':\n",
    "                index = -1\n",
    "    pre_processed_text = ' '.join(pre_processed_text)\n",
    "    return pre_processed_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet: EN-His\n",
      "  Male: [1, 4, 5, 7, 8, 9, 11, 12, 14, 16, 19, 20, 21, 22, 23, 24]\n",
      "  Female: [2, 3, 6, 10, 13, 15, 17, 18, 25]\n",
      "\n",
      "Sheet: EN-Psy\n",
      "  Male: [1, 2, 3, 4, 10, 14, 16, 17, 19, 20, 23, 24, 25, 26, 27, 28, 29]\n",
      "  Female: [5, 6, 7, 8, 9, 11, 12, 13, 15, 18, 21, 22, 30]\n",
      "\n",
      "Sheet: FR-His\n",
      "  Male: [2, 9, 12, 16, 18, 19, 21, 23, 24]\n",
      "  Female: [1, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 20, 22, 25]\n",
      "\n",
      "Sheet: FR-Psy\n",
      "  Male: [1, 2, 3, 8, 9, 10, 11, 12, 14, 15, 20, 21, 22, 24, 25, 26]\n",
      "  Female: [4, 5, 6, 7, 13, 16, 17, 18, 19, 23, 27]\n",
      "\n",
      "Sheet: ES-His\n",
      "  Male: [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "  Female: [2, 13, 14, 16]\n",
      "\n",
      "Sheet: ES-Psy\n",
      "  Male: [3, 4, 8, 12, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25]\n",
      "  Female: [1, 2, 5, 6, 7, 9, 10, 11, 13, 21, 24]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "xlsx_path = \"Metadata.xlsx\"\n",
    "excel_data = pd.ExcelFile(xlsx_path)\n",
    "\n",
    "# Dictionary to store gender arrays for each sheet\n",
    "gender_data = {}\n",
    "\n",
    "for sheet in excel_data.sheet_names:\n",
    "    \n",
    "    df = excel_data.parse(sheet)\n",
    "    \n",
    "    # Drop rows where Gender or FileName is missing\n",
    "    df = df.dropna(subset=[\"Gender\", \"FileName\"])\n",
    "    \n",
    "    # Ensure FileName is integer for consistency\n",
    "    df[\"FileName\"] = df[\"FileName\"].astype(int)\n",
    "    \n",
    "    male_files = df[df[\"Gender\"] == \"M\"][\"FileName\"].tolist()\n",
    "    female_files = df[df[\"Gender\"] == \"F\"][\"FileName\"].tolist()\n",
    "    \n",
    "    gender_data[sheet] = {\n",
    "        \"Male\": male_files,\n",
    "        \"Female\": female_files\n",
    "    }\n",
    "\n",
    "# Print gender arrays for each sheet\n",
    "for sheet, genders in gender_data.items():\n",
    "    print(f\"Sheet: {sheet}\")\n",
    "    print(\"  Male:\", genders[\"Male\"])\n",
    "    print(\"  Female:\", genders[\"Female\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n",
      "EN\n",
      "FR\n",
      "FR\n",
      "ES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES\n"
     ]
    }
   ],
   "source": [
    "Dataset = {}\n",
    "for sheet, genders in gender_data.items():\n",
    "    Dataset[sheet] = {}\n",
    "    print(sheet.split('-')[0])\n",
    "    if(sheet.split('-')[0])=='EN':\n",
    "        nlp = spacy.load(\"en_core_web_sm\") \n",
    "        start = 0\n",
    "    elif sheet.split('-')[0] == 'FR':\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        start = 1\n",
    "    else:\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "        start = 1\n",
    "    male = genders['Male']\n",
    "    female = genders['Female']\n",
    "    Dataset[sheet]['Male'] = []\n",
    "    Dataset[sheet]['Female'] = []\n",
    "    for file in male:\n",
    "        if os.path.exists('./' + sheet + '/' + str(file) + '.pdf'):\n",
    "            temp = extract_text_from_pdf('./' + sheet + '/' + str(file) + '.pdf', start)\n",
    "            Dataset[sheet]['Male'].append([temp, file])\n",
    "    for file in female:\n",
    "        if os.path.exists('./' + sheet + '/' + str(file) + '.pdf'):\n",
    "            temp = extract_text_from_pdf('./' + sheet + '/' + str(file) + '.pdf', start)\n",
    "            Dataset[sheet]['Female'].append([temp, file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, language):\n",
    "    return word_tokenize(text, language=language)\n",
    "def calculate_ttr(text, language):\n",
    "    tokens = tokenize_text(text, language)\n",
    "    unique_tokens = set(tokens)\n",
    "    return len(unique_tokens) / len(tokens) if tokens else 0\n",
    "\n",
    "def calculate_hapax_legomena(text, language):\n",
    "    tokens = tokenize_text(text, language)\n",
    "    word_counts = Counter(tokens)\n",
    "    hapax_legomena = sum(1 for word in word_counts if word_counts[word] == 1)\n",
    "    return hapax_legomena / len(tokens) if tokens else 0\n",
    "\n",
    "def calculate_msl(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    total_words = len([token.text for token in doc if not token.is_punct])\n",
    "    return total_words / len(sentences) if sentences else 0\n",
    "\n",
    "def flesch_Kincaid_Readability_Score(text):\n",
    "    doc = nlp(text)\n",
    "    total_sentences = len(list(doc.sents))\n",
    "    total_words = len([token.text for token in doc if not token.is_punct])\n",
    "    total_syllables = sum([len(list(token._.syllables)) for token in doc if not token.is_punct])\n",
    "    \n",
    "    if total_words == 0 or total_sentences == 0:\n",
    "        return 0\n",
    "    \n",
    "    score = 206.835 - (1.015 * (total_words / total_sentences)) - (84.6 * (total_syllables / total_words))\n",
    "    return score\n",
    "\n",
    "def pronoun_usage(text):\n",
    "    doc = nlp(text)\n",
    "    pronouns = [token.text for token in doc if token.pos_ == \"PRON\"]\n",
    "    pronoun_counts = Counter(pronouns)\n",
    "    return dict(pronoun_counts)\n",
    "def chi_square_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_punct]\n",
    "    word_counts = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    expected_freq = total_words / len(word_counts) if word_counts else 0\n",
    "    chi_square = sum((observed - expected_freq) ** 2 / expected_freq for observed in word_counts.values() if observed > 0)\n",
    "    return chi_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from gender_guesser.detector import Detector\n",
    "\n",
    "gender_detector = Detector()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "def extract_in_text_citations(text):\n",
    "    \"\"\"Extract both author-year and numeric citations\"\"\"\n",
    "    citations = []\n",
    "    author_year = re.findall(r'\\(([A-Z][a-z]+(?:,\\s\\d{4}| et al., \\d{4}| & [A-Z][a-z]+, \\d{4}))\\)', text)\n",
    "    numeric = re.findall(r'\\[(\\d+(?:-\\d+)?)\\]', text)\n",
    "    return author_year + numeric\n",
    "\n",
    "def get_first_author(reference_text):\n",
    "    inputs = tokenizer(reference_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    authors = []\n",
    "    current_author = []\n",
    "    \n",
    "    for token, pred in zip(tokens, predictions[0].numpy()):\n",
    "        label = model.config.id2label[pred]\n",
    "        if label == \"B-PER\":\n",
    "            if current_author:\n",
    "                authors.append(\" \".join(current_author))\n",
    "                break \n",
    "            current_author = [token.replace(\"##\", \"\")]\n",
    "        elif label == \"I-PER\" and current_author:\n",
    "            current_author.append(token.replace(\"##\", \"\"))\n",
    "    \n",
    "    return authors[0] if authors else None\n",
    "\n",
    "def analyze_citations(text, language=\"english\"):\n",
    "    citations = extract_in_text_citations(text)\n",
    "    \n",
    "    # Define hedging phrases for each supported language\n",
    "    hedging_phrases = {\n",
    "        'english': {'suggest', 'may', 'might', 'could', 'possibly'},\n",
    "        'french': {'suggère', 'peut', 'pourrait', 'éventuellement', 'probablement'},\n",
    "        'spanish': {'sugiere', 'puede', 'podría', 'posiblemente', 'quizás'}\n",
    "    }\n",
    "    \n",
    "    # Select the appropriate set of phrases based on the language parameter\n",
    "    selected_phrases = hedging_phrases.get(language.lower(), hedging_phrases['english'])\n",
    "    \n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    hedging_cites = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Check if the sentence contains any citation\n",
    "        if any(cite in sent for cite in citations):\n",
    "            # Check for presence of any hedging phrase in the sentence (case-insensitive)\n",
    "            if any(phrase in sent.lower() for phrase in selected_phrases):\n",
    "                hedging_cites += 1\n",
    "    \n",
    "    # Calculate the Citation Uncertainty Rate (CUR)\n",
    "    cur = hedging_cites / len(citations) if citations else 0\n",
    "    \n",
    "    return {\n",
    "        'total_citations': len(citations),\n",
    "        'cur': cur\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN-His\n",
      "TTR:-  0.3248142880787969 0.2714959907416341\n",
      "Hapax Legomena:-  0.25043326068202126 0.19665579673174652\n",
      "MSL:-  16.09588387184193 21.9951025394151\n",
      "CUR:- 0.3958333333333333 0.04398148148148148\n",
      "Chi Square Text:-  511886.1919102939 668526.6825014201\n",
      "EN-Psy\n",
      "TTR:-  0.21261659129623015 0.2131323496472725\n",
      "Hapax Legomena:-  0.12623788183266757 0.12657058226128257\n",
      "MSL:-  20.616088359153014 21.547548514604834\n",
      "CUR:- 0.12357272038626946 0.009505593875866882\n",
      "Chi Square Text:-  437174.03284629446 573672.1753473484\n",
      "FR-His\n",
      "TTR:-  0.23375645011856594 0.2530662466288745\n",
      "Hapax Legomena:-  0.149179473271893 0.16478087087262272\n",
      "MSL:-  16.943398788426375 16.801288622327277\n",
      "CUR:- 0.0 0.0\n",
      "Chi Square Text:-  314139.21807099873 272418.3990306876\n",
      "FR-Psy\n",
      "TTR:-  0.2241803381696998 0.20460729131691693\n",
      "Hapax Legomena:-  0.13740116603071093 0.12264160162581268\n",
      "MSL:-  16.521578359310404 17.853652820681223\n",
      "CUR:- 0.16168776864403797 0.014698888058548907\n",
      "Chi Square Text:-  258327.49852094435 313955.9297096981\n",
      "ES-His\n",
      "TTR:-  0.2805809367336477 0.2878076732108588\n",
      "Hapax Legomena:-  0.20829314233609142 0.20047050759013912\n",
      "MSL:-  19.835816150856523 24.741903470130797\n",
      "CUR:- 0.0 0.0\n",
      "Chi Square Text:-  593966.0273669346 210586.02092838858\n",
      "ES-Psy\n",
      "TTR:-  0.25165931979548156 0.24101663385450958\n",
      "Hapax Legomena:-  0.16186760457422367 0.14968488274377806\n",
      "MSL:-  24.457296199725253 22.25917496412609\n",
      "CUR:- 0.056543697848045686 0.007067962231005711\n",
      "Chi Square Text:-  368721.3964317331 293283.357850407\n"
     ]
    }
   ],
   "source": [
    "for sheet in Dataset:\n",
    "    if(sheet.split('-')[0])=='EN':\n",
    "        language = \"english\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    elif sheet.split('-')[0] == 'FR':\n",
    "        language = \"french\"\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    else:\n",
    "        language = \"spanish\"\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "    ttr_m = 0\n",
    "    ttr_f = 0\n",
    "    hapax_legomena_m = 0\n",
    "    hapax_legomena_f = 0\n",
    "    msl_m = 0\n",
    "    msl_f = 0\n",
    "    fkrs_m = 0\n",
    "    fkrs_f = 0\n",
    "    cst_m = 0\n",
    "    cst_f = 0\n",
    "    cur_f = 0\n",
    "    cur_m = 0\n",
    "    print(sheet)\n",
    "    for text in Dataset[sheet]['Male']:\n",
    "        ttr_m += calculate_ttr(text[0], language)\n",
    "        hapax_legomena_m += calculate_hapax_legomena(text[0],  language)\n",
    "        msl_m += calculate_msl(text[0])\n",
    "        cur_m += analyze_citations(text[0], language)['cur']\n",
    "        #fkrs_m += flesch_Kincaid_Readability_Score(text)\n",
    "        cst_m += chi_square_text(text[0])\n",
    "    for text in Dataset[sheet]['Female']:\n",
    "        ttr_f += calculate_ttr(text[0], language)\n",
    "        hapax_legomena_f += calculate_hapax_legomena(text[0],  language)\n",
    "        msl_f += calculate_msl(text[0])\n",
    "        #fkrs_f += flesch_Kincaid_Readability_Score(text)\n",
    "        cst_f += chi_square_text(text[0])\n",
    "        cur_f += analyze_citations(text[0],language)['cur']\n",
    "    ttr_m = ttr_m/len(Dataset[sheet]['Male'])\n",
    "    hapax_legomena_m = hapax_legomena_m/len(Dataset[sheet]['Male'])\n",
    "    msl_m = msl_m/len(Dataset[sheet]['Male'])\n",
    "    #fkrs_m = fkrs_m/len(Dataset[sheet]['Male'])\n",
    "    cst_m = cst_m/len(Dataset[sheet]['Male'])\n",
    "    cur_m = cur_m/len(Dataset[sheet]['Male'])\n",
    "    ttr_f = ttr_f/len(Dataset[sheet]['Female'])\n",
    "    hapax_legomena_f = hapax_legomena_f/len(Dataset[sheet]['Female'])\n",
    "    msl_f = msl_f/len(Dataset[sheet]['Female'])\n",
    "    #fkrs_f = fkrs_f/len(Dataset[sheet]['Female'])\n",
    "    cst_f = cst_f/len(Dataset[sheet]['Female'])\n",
    "    cur_f = cur_m/len(Dataset[sheet]['Female'])\n",
    "    print(\"TTR:- \",ttr_m,ttr_f)\n",
    "    print(\"Hapax Legomena:- \",hapax_legomena_m,hapax_legomena_f)\n",
    "    print(\"MSL:- \",msl_m,msl_f)\n",
    "    print(\"CUR:-\", cur_f, cur_m)\n",
    "    print(\"Chi Square Text:- \", cst_m,cst_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sheet: EN-His\n",
      "Processing sheet: EN-Psy\n",
      "Processing sheet: FR-His\n",
      "Processing sheet: FR-Psy\n",
      "Processing sheet: ES-His\n",
      "Processing sheet: ES-Psy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for sheet in Dataset:\n",
    "    prefix = sheet.split('-')[0]\n",
    "    if prefix == 'EN':\n",
    "        language = \"english\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    elif prefix == 'FR':\n",
    "        language = \"french\"\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    else:\n",
    "        language = \"spanish\"\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    print(f\"Processing sheet: {sheet}\")\n",
    "\n",
    "    for gender in ['Male', 'Female']:\n",
    "        for file_name, text in enumerate(Dataset[sheet][gender]):\n",
    "            result = {\n",
    "                'Sheet': sheet,\n",
    "                'File': str(text[1])+'.pdf',  # Or replace with actual filename if available\n",
    "                'Gender': gender,\n",
    "                'TTR': calculate_ttr(text[0], language),\n",
    "                'Hapax_Legomena': calculate_hapax_legomena(text[0], language),\n",
    "                'MSL': calculate_msl(text[0]),\n",
    "                'CST': chi_square_text(text[0]),\n",
    "                'CUR': analyze_citations(text[0], language)['cur'],\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "#Optional: Save to CSV\n",
    "df_results.to_csv(\"file_level_text_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
